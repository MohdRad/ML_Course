{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2e8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\mirad\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Modify it if you have more gpus, e.g., \"0,1,2,3\" if you have 4 GPUs\n",
    "os.environ[\"NCCL_P2P_DISABLE\"]=\"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"1\"\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "import warnings\n",
    "from transformers import (AutoModelForSequenceClassification, \n",
    "                          BitsAndBytesConfig)\n",
    "from transformers import DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "# login(token='') # Create a token in your huggingface account and use it here it\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from peft import (LoraConfig, \n",
    "                  PeftConfig, \n",
    "                  get_peft_model, \n",
    "                  prepare_model_for_kbit_training,\n",
    "                  PeftModel) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn.functional as F\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "output_dir = \"llama2_sa\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00f07aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84446a25b148c6b18a6d4285a5376e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb254f54f0d495d8987db234c0ee51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Preperation\n",
    "df = pd.read_csv('zero_shot.csv')\n",
    "df = df.sample(1000)\n",
    "test = pd.read_csv('test_gen.csv')\n",
    "test = test.sample(3000)\n",
    "df_tr, df_te = train_test_split(df, test_size=0.2)\n",
    "train_dataset = Dataset.from_dict(df_tr)\n",
    "test_dataset = Dataset.from_dict(df_te)\n",
    "my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\n",
    "                                        \"test\":test_dataset})\n",
    "\n",
    "# Tokenizer definition\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Function to tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "# Use the function\n",
    "tokenized_text = my_dataset_dict.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b818cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da83829a03403fb90cb5d8ca73fb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,178,112 || all params: 6,632,538,112 || trainable%: 0.37961503688077114\n"
     ]
    }
   ],
   "source": [
    "# Define labels\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "#Quantization Configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto')\n",
    "\n",
    "# to determine target_modeuls, use print(model) to identify \"Linear\" layers\n",
    "# use the name between parentheses\n",
    "lora_config = LoraConfig(\n",
    "    r = 24, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e5418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use accuarcy for evaluation metrics \n",
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy':accuracy_score(predictions,labels)}\n",
    "\n",
    "# A custom trainer for llama2-7B\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, \n",
    "            dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee666b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: tensorflow, torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'C:\\\\Users\\\\mirad' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/mohdrad/huggingface/1b37a8270a1f4eee98460613c31c97e0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 52:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.897800</td>\n",
       "      <td>1.894150</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.286000</td>\n",
       "      <td>1.396044</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.267000</td>\n",
       "      <td>1.346163</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.276300</td>\n",
       "      <td>1.323793</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.221300</td>\n",
       "      <td>1.319913</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : violet_worm_6288\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/mohdrad/huggingface/1b37a8270a1f4eee98460613c31c97e0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [16]                     : (0.5, 5.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/accuracy [5]              : (0.325, 0.365)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/loss [5]                  : (1.3199130296707153, 1.8941497802734375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/runtime [5]               : (31.2526, 31.4584)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/samples_per_second [5]    : (6.358, 6.399)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/steps_per_second [5]      : (0.413, 0.416)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_accuracy [5]              : (0.325, 0.365)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_loss [5]                  : (1.3199130296707153, 1.8941497802734375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_runtime [5]               : (31.2526, 31.4584)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_samples_per_second [5]    : (6.358, 6.399)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_steps_per_second [5]      : (0.413, 0.416)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     grad_norm [10]                 : (14.848877906799316, 39.34071350097656)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate [10]             : (0.0, 9e-06)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [10]                      : (1.2213, 2.2884)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos                     : 1.2209433595871232e+16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/epoch [16]               : (0.5, 5.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/grad_norm [10]           : (14.848877906799316, 39.34071350097656)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/learning_rate [10]       : (0.0, 9e-06)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/loss [10]                : (1.2213, 2.2884)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/total_flos               : 1.2209433595871232e+16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_loss               : 1.4676988677978515\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_runtime            : 3175.9969\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_samples_per_second : 1.259\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_steps_per_second   : 0.079\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss                     : 1.4676988677978515\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime                  : 3175.9969\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second       : 1.259\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second         : 0.079\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                     : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/accelerator_config                 : AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, gradient_accumulation_kwargs=None)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                         : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                       : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                               : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_persistent_workers      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_prefetch_factor         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                        : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                  : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                              : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                      : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                             : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dispatch_batches                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                  : Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                    : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_do_concat_batches             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                : IntervalStrategy.EPOCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                     : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                          : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                               : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                        : {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing_kwargs      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend             : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_always_push                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                          : None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_num_input_tokens_seen      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_tokens_per_second          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                      : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                 : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                          : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                  : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                        : llama2_sa\\runs\\Dec30_13-38-01_KUMO\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                      : 25\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                   : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_kwargs                : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                          : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                      : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/neftune_noise_alpha                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                   : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                              : OptimizerNames.ADAMW_TORCH\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_target_modules               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                         : llama2_sa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                      : ParallelMode.NOT_PARALLEL\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                         : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size         : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size        : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                          : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                          : ['comet_ml', 'tensorboard']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                           : llama2_sa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_only_model                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                         : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                      : IntervalStrategy.NO\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                               : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/split_batches                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                   : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                       : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                       : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation             : sdpa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_attn_implementation_internal    : sdpa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                     : 01c7f73d771dfac7d292323805ebc428287df4f9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                    : meta-llama/Llama-2-7b-hf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_pre_quantization_dtype          : torch.float16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                    : ['LlamaForCausalLM']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_bias                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_dropout                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                    : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                     : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_act                       : silu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_size                      : 4096\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                         : {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/intermediate_size                : 11008\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/keys_to_ignore_at_inference      : ['past_key_values']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                         : {'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                       : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_position_embeddings          : 4096\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                       : llama\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                     : meta-llama/Llama-2-7b-hf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_attention_heads              : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_hidden_layers                : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_key_value_heads              : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                       : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                     : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pretraining_tp                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                     : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/quantization_config              : BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty               : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/rms_norm_eps                     : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/rope_scaling                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/rope_theta                       : 10000.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                      : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                            : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                      : torch.float16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version             : 4.31.0.dev0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                       : 32001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: tensorflow, torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning\n",
    "# Skip this block if already have done the fine tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 25,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    save_strategy=\"no\")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_text[\"train\"],\n",
    "    eval_dataset=tokenized_text[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)\n",
    "# Fine tuning\n",
    "trainer.train()\n",
    "# save the trained model \n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f1aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForSequenceClassification(\n",
       "      (base_model): LoraModel(\n",
       "        (model): LlamaForSequenceClassification(\n",
       "          (model): LlamaModel(\n",
       "            (embed_tokens): Embedding(32001, 4096)\n",
       "            (layers): ModuleList(\n",
       "              (0-31): 32 x LlamaDecoderLayer(\n",
       "                (self_attn): LlamaSdpaAttention(\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=24, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=24, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=24, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=24, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=24, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=24, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=24, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=24, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (rotary_emb): LlamaRotaryEmbedding()\n",
       "                )\n",
       "                (mlp): LlamaMLP(\n",
       "                  (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "                (input_layernorm): LlamaRMSNorm()\n",
       "                (post_attention_layernorm): LlamaRMSNorm()\n",
       "              )\n",
       "            )\n",
       "            (norm): LlamaRMSNorm()\n",
       "          )\n",
       "          (score): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved model \n",
    "checkpoint2 = './llama2_sa'\n",
    "config = PeftConfig.from_pretrained(checkpoint2)\n",
    "model = PeftModel.from_pretrained(model, checkpoint2, is_trainable=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa21c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n",
      "2176\n",
      "2304\n",
      "2432\n",
      "2560\n",
      "2688\n",
      "2816\n",
      "2944\n",
      "3072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.24      0.30      1084\n",
      "     neutral       0.29      0.56      0.38       846\n",
      "    positive       0.35      0.23      0.28      1070\n",
      "\n",
      "    accuracy                           0.33      3000\n",
      "   macro avg       0.35      0.34      0.32      3000\n",
      "weighted avg       0.35      0.33      0.32      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a testing set\n",
    "def generate_predictions(model,df_test, bs):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = bs  \n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_sentences, \n",
    "                           return_tensors=\"pt\", \n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=1024)\n",
    "\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "        print(i+batch_size)\n",
    "       \n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "\n",
    "# call the function    \n",
    "generate_predictions(model, test, 128)\n",
    "y_true = test['label'] \n",
    "y_pred = test['predictions']\n",
    "\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
