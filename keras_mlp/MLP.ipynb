{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF3cGH5OJoPRlQk7f8ffvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohdRad/ML_Course/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following tutorial, we will learn to how implement MultiLayer Perceptron Neural Networks using keras"
      ],
      "metadata": {
        "id": "v3E_4mz2z3VQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "mrTvob1nztVF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras import models # NN\n",
        "from keras import layers # NN\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, RMSprop, Adadelta, Nadam\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import mean_squared_error, r2_score # MSE\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data, define X and y, and scale them"
      ],
      "metadata": {
        "id": "mKuh4i-e1p0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data and shuffle it (randomize it)\n",
        "df = shuffle(pd.read_csv('housing.csv'))\n",
        "# Display the data\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RPcNeztN1o9m",
        "outputId": "8c307019-2027-433c-f69b-9a0b69c00b66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  TAX  \\\n",
              "186   0.05602   0.0   2.46     0  0.488  7.831   53.6  3.1992    3  193   \n",
              "435  11.16040   0.0  18.10     0  0.740  6.629   94.6  2.1247   24  666   \n",
              "27    0.95577   0.0   8.14     0  0.538  6.047   88.8  4.4534    4  307   \n",
              "75    0.09512   0.0  12.83     0  0.437  6.286   45.0  4.5026    5  398   \n",
              "109   0.26363   0.0   8.56     0  0.520  6.229   91.2  2.5451    5  384   \n",
              "..        ...   ...    ...   ...    ...    ...    ...     ...  ...  ...   \n",
              "346   0.06162   0.0   4.39     0  0.442  5.898   52.3  8.0136    3  352   \n",
              "50    0.08873  21.0   5.64     0  0.439  5.963   45.7  6.8147    4  243   \n",
              "480   5.82401   0.0  18.10     0  0.532  6.242   64.7  3.4242   24  666   \n",
              "290   0.03502  80.0   4.95     0  0.411  6.861   27.9  5.1167    4  245   \n",
              "386  24.39380   0.0  18.10     0  0.700  4.652  100.0  1.4672   24  666   \n",
              "\n",
              "     PTRATIO       B  LSTAT  MEDV  \n",
              "186     17.8  392.63   4.45  50.0  \n",
              "435     20.2  109.85  23.27  13.4  \n",
              "27      21.0  306.38  17.28  14.8  \n",
              "75      18.7  383.23   8.94  21.4  \n",
              "109     20.9  391.23  15.55  19.4  \n",
              "..       ...     ...    ...   ...  \n",
              "346     18.8  364.61  12.67  17.2  \n",
              "50      16.8  395.56  13.45  19.7  \n",
              "480     20.2  396.90  10.74  23.0  \n",
              "290     19.2  396.90   3.33  28.5  \n",
              "386     20.2  396.90  28.28  10.5  \n",
              "\n",
              "[506 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e8856b6-eb38-46fc-a13b-fa907b534bb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>0.05602</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0</td>\n",
              "      <td>0.488</td>\n",
              "      <td>7.831</td>\n",
              "      <td>53.6</td>\n",
              "      <td>3.1992</td>\n",
              "      <td>3</td>\n",
              "      <td>193</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.63</td>\n",
              "      <td>4.45</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>11.16040</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.740</td>\n",
              "      <td>6.629</td>\n",
              "      <td>94.6</td>\n",
              "      <td>2.1247</td>\n",
              "      <td>24</td>\n",
              "      <td>666</td>\n",
              "      <td>20.2</td>\n",
              "      <td>109.85</td>\n",
              "      <td>23.27</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.95577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.047</td>\n",
              "      <td>88.8</td>\n",
              "      <td>4.4534</td>\n",
              "      <td>4</td>\n",
              "      <td>307</td>\n",
              "      <td>21.0</td>\n",
              "      <td>306.38</td>\n",
              "      <td>17.28</td>\n",
              "      <td>14.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.09512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0</td>\n",
              "      <td>0.437</td>\n",
              "      <td>6.286</td>\n",
              "      <td>45.0</td>\n",
              "      <td>4.5026</td>\n",
              "      <td>5</td>\n",
              "      <td>398</td>\n",
              "      <td>18.7</td>\n",
              "      <td>383.23</td>\n",
              "      <td>8.94</td>\n",
              "      <td>21.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.26363</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.56</td>\n",
              "      <td>0</td>\n",
              "      <td>0.520</td>\n",
              "      <td>6.229</td>\n",
              "      <td>91.2</td>\n",
              "      <td>2.5451</td>\n",
              "      <td>5</td>\n",
              "      <td>384</td>\n",
              "      <td>20.9</td>\n",
              "      <td>391.23</td>\n",
              "      <td>15.55</td>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>0.06162</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0</td>\n",
              "      <td>0.442</td>\n",
              "      <td>5.898</td>\n",
              "      <td>52.3</td>\n",
              "      <td>8.0136</td>\n",
              "      <td>3</td>\n",
              "      <td>352</td>\n",
              "      <td>18.8</td>\n",
              "      <td>364.61</td>\n",
              "      <td>12.67</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.08873</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.64</td>\n",
              "      <td>0</td>\n",
              "      <td>0.439</td>\n",
              "      <td>5.963</td>\n",
              "      <td>45.7</td>\n",
              "      <td>6.8147</td>\n",
              "      <td>4</td>\n",
              "      <td>243</td>\n",
              "      <td>16.8</td>\n",
              "      <td>395.56</td>\n",
              "      <td>13.45</td>\n",
              "      <td>19.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>5.82401</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532</td>\n",
              "      <td>6.242</td>\n",
              "      <td>64.7</td>\n",
              "      <td>3.4242</td>\n",
              "      <td>24</td>\n",
              "      <td>666</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>10.74</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0.03502</td>\n",
              "      <td>80.0</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0</td>\n",
              "      <td>0.411</td>\n",
              "      <td>6.861</td>\n",
              "      <td>27.9</td>\n",
              "      <td>5.1167</td>\n",
              "      <td>4</td>\n",
              "      <td>245</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>3.33</td>\n",
              "      <td>28.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>24.39380</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.700</td>\n",
              "      <td>4.652</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.4672</td>\n",
              "      <td>24</td>\n",
              "      <td>666</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>28.28</td>\n",
              "      <td>10.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e8856b6-eb38-46fc-a13b-fa907b534bb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e8856b6-eb38-46fc-a13b-fa907b534bb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e8856b6-eb38-46fc-a13b-fa907b534bb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6d6b6c09-ea9b-4fcb-ab46-48d95d1cd11b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d6b6c09-ea9b-4fcb-ab46-48d95d1cd11b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6d6b6c09-ea9b-4fcb-ab46-48d95d1cd11b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b7b4f94f-51ce-4210-beb3-9cc49a5ec9ac\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b7b4f94f-51ce-4210-beb3-9cc49a5ec9ac button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 506,\n  \"fields\": [\n    {\n      \"column\": \"CRIM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.60154510533249,\n        \"min\": 0.00632,\n        \"max\": 88.9762,\n        \"num_unique_values\": 504,\n        \"samples\": [\n          0.06899,\n          0.07165,\n          0.13914\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ZN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.32245299451514,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          30.0,\n          90.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INDUS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.860352940897585,\n        \"min\": 0.46,\n        \"max\": 27.74,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          8.56,\n          6.06,\n          27.74\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHAS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NOX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11587767566755595,\n        \"min\": 0.385,\n        \"max\": 0.871,\n        \"num_unique_values\": 81,\n        \"samples\": [\n          0.507,\n          0.488\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7026171434153233,\n        \"min\": 3.561,\n        \"max\": 8.78,\n        \"num_unique_values\": 446,\n        \"samples\": [\n          6.382,\n          6.442\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.148861406903617,\n        \"min\": 2.9,\n        \"max\": 100.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          35.7,\n          42.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.105710126627611,\n        \"min\": 1.1296,\n        \"max\": 12.1265,\n        \"num_unique_values\": 412,\n        \"samples\": [\n          6.8185,\n          6.498\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 24,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          6,\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 168,\n        \"min\": 187,\n        \"max\": 711,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          270,\n          198\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PTRATIO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1649455237144406,\n        \"min\": 12.6,\n        \"max\": 22.0,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          17.0,\n          18.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91.29486438415783,\n        \"min\": 0.32,\n        \"max\": 396.9,\n        \"num_unique_values\": 357,\n        \"samples\": [\n          390.07,\n          395.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LSTAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.14106151134857,\n        \"min\": 1.73,\n        \"max\": 37.97,\n        \"num_unique_values\": 455,\n        \"samples\": [\n          6.65,\n          16.35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEDV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.197104087379817,\n        \"min\": 5.0,\n        \"max\": 50.0,\n        \"num_unique_values\": 229,\n        \"samples\": [\n          27.0,\n          20.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df.drop(['MEDV'], axis=1))\n",
        "# The output is MEDV\n",
        "y = np.array(df['MEDV'])\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "L = int(0.8*len(X))\n",
        "X_train = X[:L]\n",
        "y_train = y[:L]\n",
        "X_test = X[L:]\n",
        "y_test = y[L:]\n",
        "\n",
        "# X scaler\n",
        "X_scaler = MinMaxScaler()\n",
        "# Let the scaler defines the maximum and the minimum\n",
        "X_scaler.fit(X_train)\n",
        "# Apply scaling to train X data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "# test data\n",
        "X_scaler.fit(X_test)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "# y scaler\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(y_train)\n",
        "y_train_scaled = y_scaler.transform(y_train)\n",
        "y_scaler.fit(y_test)\n",
        "# No need to scale the y_val, it will not and should not be used by the ML model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Bfzp8_i53j_D",
        "outputId": "a4396dd2-e7b1-43c0-92ef-36453e618a0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the neural network (NN). The smallest NN is called Artificial Neural Network (ANN). This network has 1 input layer, 1 hidden layer, and 1 output layer. If you use more than 1 hidden layer and up to 3, this is called MLP, when you use 4 and more, this is called Deep Neural Network (DNN). We will define MLP with two hidden layers"
      ],
      "metadata": {
        "id": "IygaaUkb383K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "# Input layer\n",
        "# assign the number of input features to n to be used in the input layer\n",
        "# m is the number of rows and n is the number of columns of X_train\n",
        "m,n = np.shape(X_train)\n",
        "# Input layer, here we will use the variable n for number of hidden units\n",
        "model.add(layers.Dense(n,              # Number of hidden units == number of input features\n",
        "                   activation='tanh',  # Activation function, 'sigmoid' and 'relu' are other choices\n",
        "                   input_shape=(n,)))  # The dimensions of the input = n\n",
        "# Hidden Layers\n",
        "# Layer 1\n",
        "model.add(layers.Dense(10,\n",
        "                   activation='tanh',\n",
        "                   kernel_initializer='normal'))\n",
        "# Layer 2\n",
        "model.add(layers.Dense(5,\n",
        "                       activation='tanh',\n",
        "                       kernel_initializer='normal'))\n",
        "# Output layer\n",
        "model.add(layers.Dense(1,               # Number of hidden units equals number of outputs\n",
        "                   activation='tanh'))\n",
        "\n",
        "# After a model is created, you must call its compile() method to specify the\n",
        "# loss function is mean squraed error (mse) and the optimizer is adam (most common).\n",
        "model.compile(loss= 'mse', optimizer= 'adam')\n",
        "# Training the Model\n",
        "history = model.fit(X_train_scaled,\n",
        "                    y_train_scaled,\n",
        "                    epochs=500,           # Number of iterations\n",
        "                    batch_size=10,        # Number of samples in each batch. Number of batches is the number of training samples / Number of samples in each batch\n",
        "                    verbose=1,            # Print the progress\n",
        "                    validation_split=0.1, # fraction of the training data that is used for validation, this is to generate validation learning curve to inspect overfitting.\n",
        "                    shuffle=False)        # put it False since we shuffled the data and to get smooth learning curves.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0-7bxdqr3lBp",
        "outputId": "44f7bf43-d5af-48a0-bb9b-40d5e8135886"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 1s 8ms/step - loss: 0.1364 - val_loss: 0.0837\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.0498\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0392\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0327 - val_loss: 0.0296\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0252\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0233\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0216\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0199\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0182\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 2s 42ms/step - loss: 0.0184 - val_loss: 0.0166\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0170 - val_loss: 0.0154\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.0145\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0139\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0146 - val_loss: 0.0134\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0142 - val_loss: 0.0131\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0140 - val_loss: 0.0128\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0137 - val_loss: 0.0126\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0124\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0134 - val_loss: 0.0123\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 2s 32ms/step - loss: 0.0132 - val_loss: 0.0121\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0131 - val_loss: 0.0120\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0119\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0129 - val_loss: 0.0118\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0117\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0128 - val_loss: 0.0117\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0127 - val_loss: 0.0116\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0115\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0126 - val_loss: 0.0115\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0125 - val_loss: 0.0114\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0113\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0113\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0112\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0123 - val_loss: 0.0112\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0123 - val_loss: 0.0111\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0122 - val_loss: 0.0111\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0122 - val_loss: 0.0110\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0121 - val_loss: 0.0110\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0109\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0109\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0109\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0120 - val_loss: 0.0108\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0108\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0107\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0107\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0107\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0118 - val_loss: 0.0106\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0118 - val_loss: 0.0106\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0117 - val_loss: 0.0105\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0117 - val_loss: 0.0105\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0105\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0104\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0116 - val_loss: 0.0104\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0103\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 0.0115 - val_loss: 0.0103\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0103\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0102\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0102\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0102\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0114 - val_loss: 0.0101\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0101\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0113 - val_loss: 0.0100\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0113 - val_loss: 0.0100\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 1s 3ms/step - loss: 0.0112 - val_loss: 0.0100\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0112 - val_loss: 0.0099\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0099\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0111 - val_loss: 0.0098\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0111 - val_loss: 0.0098\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0097\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0110 - val_loss: 0.0097\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0110 - val_loss: 0.0097\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0109 - val_loss: 0.0096\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0109 - val_loss: 0.0096\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0095\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0095\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0095\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0094\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0094\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 0.0107 - val_loss: 0.0093\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.0106 - val_loss: 0.0093\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0093\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.0092\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0105 - val_loss: 0.0092\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 1s 16ms/step - loss: 0.0105 - val_loss: 0.0091\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0091\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0090\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0103 - val_loss: 0.0090\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0103 - val_loss: 0.0090\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 1s 3ms/step - loss: 0.0103 - val_loss: 0.0089\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0102 - val_loss: 0.0089\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0102 - val_loss: 0.0088\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0088\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0087\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0087\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0086\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 1s 31ms/step - loss: 0.0100 - val_loss: 0.0086\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 0.0099 - val_loss: 0.0086\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 2s 31ms/step - loss: 0.0099 - val_loss: 0.0085\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0098 - val_loss: 0.0085\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0084\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.0084\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.0083\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0096 - val_loss: 0.0083\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0096 - val_loss: 0.0082\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 0.0082\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0095 - val_loss: 0.0081\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0094 - val_loss: 0.0081\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 0.0094 - val_loss: 0.0081\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 0.0093 - val_loss: 0.0080\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0093 - val_loss: 0.0080\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0092 - val_loss: 0.0079\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0092 - val_loss: 0.0079\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0091 - val_loss: 0.0078\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0091 - val_loss: 0.0078\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0078\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0090 - val_loss: 0.0077\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 0.0077\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 0.0076\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0076\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0088 - val_loss: 0.0075\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0075\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0075\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0074\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.0074\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0086 - val_loss: 0.0073\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0085 - val_loss: 0.0073\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0085 - val_loss: 0.0073\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0072\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0072\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0071\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0071\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0071\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0070\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0070\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0069\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0069\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0069\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0068\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0080 - val_loss: 0.0068\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0068\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0067\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0067\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0067\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.0066\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0066\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0066\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0077 - val_loss: 0.0065\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0065\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0065\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0064\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0076 - val_loss: 0.0064\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0075 - val_loss: 0.0064\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0063\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0063\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0063\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0074 - val_loss: 0.0063\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0062\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0062\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0062\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.0061\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0061\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.0061\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0072 - val_loss: 0.0061\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0060\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0060\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0072 - val_loss: 0.0060\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0060\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0060\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0059\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0071 - val_loss: 0.0059\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0059\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.0059\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.0059\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0070 - val_loss: 0.0058\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.0058\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.0058\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0058\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0058\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0057\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0057\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0056\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0056\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0067 - val_loss: 0.0056\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0067 - val_loss: 0.0055\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0055\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0055\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0055\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0055\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0055\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0055\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.0054\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0054\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0054\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0065 - val_loss: 0.0054\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0053\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0053\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0053\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0053\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0063 - val_loss: 0.0052\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0051\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0051\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0051\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 1s 24ms/step - loss: 0.0060 - val_loss: 0.0051\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0060 - val_loss: 0.0051\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0060 - val_loss: 0.0050\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0050\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0050\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0050\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0050\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0058 - val_loss: 0.0050\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0050\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0050\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0049\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0048\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0048\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0048\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0048\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0048\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 1s 3ms/step - loss: 0.0054 - val_loss: 0.0047\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 2s 28ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0046\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 2s 52ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 1s 30ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 1s 14ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 2s 27ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0042\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 0s 13ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0042\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 1s 31ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning curves for neural networks show the loss function (error) with epochs (iterations). The loss should decrease with epochs and both validation and training curves should stay close to each other as shown in the Figure below. **If the validation curve becomes siginificantly higher than the learning one, this is overfitting.** Note that lograthmic scale is used on the y-axis because Mean Squared Error is usually very small."
      ],
      "metadata": {
        "id": "2hkPuF4j4o_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Learning Curves\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('MLP model loss')\n",
        "plt.ylabel('MSE')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "oqfMYkvO9KvC",
        "outputId": "0f51edad-02dd-45db-8008-e1cf31d90b85"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Predictions\n",
        "model_pred_y = model.predict(X_test_scaled)\n",
        "y_pred = y_scaler.inverse_transform(model_pred_y)\n",
        "R2 = r2_score(y_test, y_pred)\n",
        "print ('R2 = ', R2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_oiTxCr5UGP",
        "outputId": "8889a9df-59e8-4f25-e0aa-e70b3d222097"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "R2 =  0.8378543412072577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The R$^2$ is comparable to Random Forest and Gradient Boosting from tutorial one.\n",
        "\n",
        "While there might be packages to do hyperparameters tuning for keras, I like to do it via a function. You can recreate the model in the 4th code block as a function and assign `epochs`, `batch_size`, `number of hidden units in the hidden layers`, `optimizer`, `activation` as variables. I will leave this to you as an excerise. Make a function to tune `epochs`, `batch_size`, and the number of hidden units in the two hidden layers (above I used 10 and 5).  "
      ],
      "metadata": {
        "id": "9JmuuAvIbob5"
      }
    }
  ]
}
